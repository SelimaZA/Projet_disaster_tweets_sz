# -*- coding: utf-8 -*-
"""Projet_NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OOcPBCdM_b6CWhiNkiC2u2pqtq5Jw4Fj
"""

import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import spacy

import numpy as np
import tensorflow as tf
import pathlib 
from tensorflow import keras
from tensorflow.keras import layers
import sys, os,io,re, csv, codecs


#spacy.load('en_core_web_sm')
from spacy.lang.en.stop_words import STOP_WORDS
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')

# Nous cahrgeons les fichiers: 
df_train = pd.read_csv('/content/train.csv')
df_test = pd.read_csv('/content/test.csv')
sample_submission = pd.read_csv('/content/sample_submission.csv')

df_train.head()

df_train.shape

df_train.info()

# on va supprimer la colonne id qui n'est pas nécessaire pour l'analyse 
df_train.drop(columns="id", axis = 1, inplace = True)
df_train.head()

#On vérifie s'il y a des doublons dans les enregistrements 
duplicates_record = df_train[df_train.duplicated(subset=['text'], keep=False)] #subset nous permet d'identifier la colonne dans laquelle on veut 
# chercher les doublons, keep = false il va nous afficher les doublons en true 
duplicates_record

#Environ 179 enregistrements en double sont trouvés. Considérons maintenant (Texte, cible) et ne gardons que le premier
#enregistrement de l'enregistrement en double et supprimez le reste
df_train.drop_duplicates(subset = ['text','target'], keep = 'first', inplace = True, ignore_index = True)
duplicates_record = df_train[df_train.duplicated(subset=['text'], keep=False)]# on refait encore une fois duplicate pr voir s'il nous reste d'autres doublons 
duplicates_record

duplicates_record.shape

#Il existe encore  divers enregistrements en doublon disponibles, mais avec une différence, 
#cette fois pour le même tweet une cible temporelle est 0 et pour une autre elle est 1. Cela semble être des 
#enregistrements corrompus et le modèle NLP peut être confus, il est donc préférable de supprimer ces enregistrements
df_train=df_train.drop_duplicates(subset = ['text'], keep = False, ignore_index = True)# ici on a utilisé le keep Flase pour
# dire on supprime tous les doublons 
df_train

df_train.shape

#il est intéressant de vérifier s'il ya des valeurs manquantes: on constate pour les 2 vles qui nous intéressent text,target y en a pas 
df_train.isna().sum()

df_train=df_train[~df_train['location'].isna()]
df_train.head()
# Je décide de travailler uniquement avec les tweets où la géo-localisation est mentionnée

df_train.shape

df_train.isna().sum()

df_train["keyword"].value_counts()

df_train['target'].value_counts()
#la target est divisé d'une manière proportionnelle c'est une bonne chose

df_test.head()

df_test.drop(columns="id", axis = 1, inplace = True)

df_test.shape

#On vérifie s'il y a des doublons dans les enregistrements dans le test 
duplicates_record = df_test[df_test.duplicated(subset=['text'], keep=False)] #subset nous permet d'identifier la colonne dans laquelle on veut 
# chercher les doublons, keep = false il va nous afficher les doublons en true 
duplicates_record.head()

duplicates_record.shape

#Environ 39 enregistrements en double sont trouvés, on garde que le premier enregistrement et on supprime le reste 
df_test.drop_duplicates(subset = ['text'], keep = 'first',inplace = True, ignore_index = True)

df_test.shape

df_test.isna().sum()

df_test=df_test[~df_test['location'].isna()]
df_test.head()

df_test.shape

df_test.isna().sum()

"""I - **Visualization**"""

# Visualisation de la distribution de la cible 
plt.style.use('seaborn')
sns.countplot(df_train['target'])
plt.title('Distribution of target', fontsize=20)

df_train['keyword'].value_counts()[0:20]

#Let's take the 20 most frequent keywords
top20 = df_train['keyword'].value_counts()[0:20].index.tolist()
#We take all the keywords and targets if they are in the top20 list
df_top20 = [(keyword,target) for keyword,target in zip(df_train['keyword'], df_train['target']) if keyword in top20]
df_top20 = pd.DataFrame(df_top20, columns = ['keyword', 'target'])

#We plot the results to see the impact of keywords on target
sns.set_theme(style="ticks")
fig, ax = plt.subplots(1,1, figsize = (20,6))
g = sns.histplot(data = df_top20, x = 'keyword', hue = 'target', multiple = 'dodge', shrink = 0.5, ax = ax)
g.set_xticklabels(labels = top20, rotation=30)
g.set_xlabel('')
g.set_title('Top 20 Keyword count by nature of tweet', fontsize=20)
g.legend(['Disaster','Not Disaster'],loc="upper right")

plt.show()

#Comme nous pouvons le voir, certains mots-clés sont plus fréquemment utilisés pour décrire les catastrophes que d'autres
#Jetons maintenant un coup d'œil à la colonne d'emplacement

df_train['location'].value_counts()[0:20]

#Let's take the rest of the 20 most frequent keywords
top20 = df_train['location'].value_counts()[0:20].index.tolist()
#We take all the locations and targets if they are in the top20 list
df_top20 = [(location,target) for location,target in zip(df_train['location'], df_train['target']) if location in top20]
df_top20 = pd.DataFrame(df_top20, columns = ['location', 'target'])

sns.set_theme(style="ticks")

fig,ax = plt.subplots(1,1, figsize = (20,6))

g20 = sns.histplot(data = df_top20, x = 'location', hue = 'target', multiple = 'dodge', shrink = 0.5, ax = ax)
g20.set_xticklabels(labels = top20, rotation=45)
g20.set_xlabel('')
g20.set_title('Top 20 location count by nature of tweet', fontsize=20)
g20.legend(['Disaster','Not Disaster'],loc="upper right")

plt.show()

#L'emplacement n'est pas cohérent comme nous pouvons le voir par exemple "USA" et "United States" ou "California" et "California, USA",
#étant utilisés en même temps, nous devrons peut-être le nettoyer un peu si nous voulons l'utiliser.

"""II- Nettoyage de la colonne localisation """

#On va sélectionner la localisation unique 
pd.DataFrame(df_train['location'].unique(), columns=['location']).sort_values(by='location')

#Après avoir vu qu'il y a des doublons, des mots mal orthographiés, l'utilisation d'abréviations et d'autres choses conduisant à des incohérences, essayons de nettoyer au mieux. 
#Je vais ensuite créer une fonction en utilisant regex

def correct_location(x):
    #special char
    x = re.sub(r"United States", "USA", x)
    x = re.sub(r"America", "USA", x)
    x = re.sub(r"America [|] New Zealand ", "USA", x)
    x = re.sub(r"Americas Newsroom", "USA", x)
    x = re.sub(r"us", "USA", x)
    x = re.sub(r"us-east-1a", "USA", x)

    x = re.sub(r"Los Angeles, CA", "Los Angeles", x)
    x = re.sub(r"Chicago, IL", "Chicago", x)

    x = re.sub(r"Boston MA", "Boston", x)
    x = re.sub(r"Boston [\]x89Û¢ Cape Cod [?]", "Boston", x)
    x = re.sub(r"Boston, MA", "Boston", x)
    x = re.sub(r"Boston, Massachusetts", "Boston", x)
    x = re.sub(r"Boston/Montreal ", "Boston", x)

    x = re.sub(r"Birmingham [&] Bristol", "Birmingham", x)
    x = re.sub(r"Birmingham UK", "Birmingham", x)
    x = re.sub(r"Birmingham and the Marches", "Birmingham", x)
    x = re.sub(r"Birmingham, England", "Birmingham", x)
    x = re.sub(r"Birmingham, UK ", "Birmingham", x)
    x = re.sub(r"Birmingham, United Kingdom", "Birmingham", x)

    x = re.sub(r"California, USA", "California", x)
    x = re.sub(r"California, United States", "California", x)
    x = re.sub(r"California or Colorado", "California", x)
    

    x = re.sub(r"New York, NY", "New York", x)
    x = re.sub(r"New York City", "New York", x)
    x = re.sub(r"New York, USA", "New York", x)
    x = re.sub(r"NY", "New York", x)
    x = re.sub(r"new york", "New York", x)
    x = re.sub(r"nc", "New York", x)
    x = re.sub(r"nyc", "New York", x)

    x = re.sub(r"Florida Forever", "Florida", x)
    x = re.sub(r"Florida USA", "Florida", x)
    x = re.sub(r"Florida but I wanna be n Texas", "Florida", x)
    x = re.sub(r"Florida, USA", "Florida", x)
    x = re.sub(r"Florida, USA", "Florida", x)

    x = re.sub(r"Dallas, TX", "Dallas", x)
    x = re.sub(r"Dallas, TX ", "Dallas", x)
    x = re.sub(r"Dallas, Tejas", "Dallas", x)
    x = re.sub(r"Dallas, Texas[.] ", "Dallas", x)

    x = re.sub(r"England ", "England", x)
    x = re.sub(r"England [&] Wales Border, UK", "England", x)
    x = re.sub(r"England, Great Britain[.]", "England", x)
    x = re.sub(r"England, United Kingdom", "England", x)
    x = re.sub(r"England,UK,Europe,Sol 3[.]", "England", x)
    x = re.sub(r"England[.]", "England", x)
    x = re.sub(r"English Midlands", "England", x)

    x = re.sub(r"Texas, USA", "Texas", x)
    x = re.sub(r"Lagos, Nigeria", "Nigeria", x)
    x = re.sub(r"Nxgerxa", "Lagos", x)

    x = re.sub(r"Tampa, FL", "Tampa", x)
    x = re.sub(r"Florida, USA", "Florida", x)
    x = re.sub(r"Asheville, NC", "Asheville", x)
    x = re.sub(r"Oklahoma City, OK ", "Oklahoma City", x)

    x = re.sub(r"Melbourne, Australia", "Australia", x)
    x = re.sub(r"Adelaide", "Australia", x)
    x = re.sub(r"Adelaide, Australia, Australia", "Australia", x)
    x = re.sub(r"Adelaide, South Australia", "Australia", x)
    x = re.sub(r"Australia ", "Australia", x)
    x = re.sub(r"Australian Capital Territory", "Australia", x)

    x = re.sub(r"Bangalore City, India", "Bangalore", x)
    x = re.sub(r"Bangalore, INDIA", "Bangalore", x)
    x = re.sub(r"Bangalore, India", "Bangalore", x)
    x = re.sub(r"Bangalore[.] India", "Bangalore", x)

    x = re.sub(r"Mumbai \(India\)", "Mumbai", x)
    x = re.sub(r"Mumbai india", "Mumbai", x)
    x = re.sub(r"Bangalore[.] India", "Mumbai", x)
    x = re.sub(r"Bangalore[.] India", "Mumbai", x)

    x = re.sub(r"Washington, DC", "Washington", x)
    x = re.sub(r"Washington, D[.]C[.]", "Washington", x)
    x = re.sub(r"Washington, D[.]C[.]", "Washington", x)
    x = re.sub(r"Washington [&] Charlotte", "Washington", x)
    x = re.sub(r"Washington 20009", "Washington", x)
    x = re.sub(r"Washington D[.]C[.]", "Washington", x)
    x = re.sub(r"Washington DC", "Washington", x)
    x = re.sub(r"Washington DC [/] Nantes", "Washington", x)
    x = re.sub(r"Washington NATIVE", "Washington", x)
    x = re.sub(r"Washington State", "Washington", x)
    x = re.sub(r"Washington state", "Washington", x)

    x = re.sub(r"San Francisco, CA", "San Francisco", x)

    x = re.sub(r"London, UK", "London", x)
    x = re.sub(r"London, England", "London", x)
    x = re.sub(r"London / Berlin / Online", "London", x)
    x = re.sub(r"London UK", "London", x)
    x = re.sub(r"London[.]", "London", x)
    x = re.sub(r"London/Bristol/Guildford", "London", x)
    x = re.sub(r"London/Lagos/FL ÌÏT: 6[.]6200132", "London", x)
    x = re.sub(r"London/New York", "London", x)
    x = re.sub(r"London/Outlaw Country", "London", x)
    x = re.sub(r"London/Surrey", "London", x)

    x = re.sub(r"Atlanta, GA", "Atlanta", x)
    x = re.sub(r"Atlanta [-] FAU class of '18", "Atlanta", x)
    x = re.sub(r"Atlanta Georgia", "Atlanta", x)
    x = re.sub(r"Atlanta Georgia ", "Atlanta", x)
    x = re.sub(r"Atlanta g[.]a[.]", "Atlanta", x)
    x = re.sub(r"Atlanta[(]ish[)], GA", "Atlanta", x)
    x = re.sub(r"Atlanta, Ga", "Atlanta", x)
    x = re.sub(r"Atlanta, Georgia", "Atlanta", x)
    x = re.sub(r"Atlanta, Georgia USA", "Atlanta", x)
    x = re.sub(r"Atlanta,Ga", "Atlanta", x)
    x = re.sub(r"Atlanta ", "Atlanta", x)
    x = re.sub(r"Atlanta USA", "Atlanta", x)
    x = re.sub(r"Atlanta[(]ish[)]", "Atlanta", x)
    x = re.sub(r"Atlanta,Ga", "Atlanta", x)

    x = re.sub(r"Sacramento, CA", "Sacramento", x)

    x = re.sub(r"Nashville, TN", "Nashville", x)
    x = re.sub(r"Denver, Colorado", "Denver", x)
    x = re.sub(r"Dallas, TX", "Dallas", x)
    x = re.sub(r"Houston, TX", "Houston", x)
    x = re.sub(r"Seattle, WA", "Seattle", x)
    x = re.sub(r"Pennsylvania, USA, WA", "Pennsylvania", x)
    x = re.sub(r"Memphis, TN", "Memphis", x)
    x = re.sub(r"Austin, TX", "Austin", x)
    x = re.sub(r"Austin TX", "Austin", x)
    x = re.sub(r"Austin [|] San Diego", "Austin", x)
    x = re.sub(r"Austin/Los Angeles", "Austin", x)

    x = re.sub(r"Portland, OR", "Portland", x)

    x = re.sub(r"Charlotte, NC", "Charlotte", x)
    x = re.sub(r"Charlotte ", "Charlotte", x)
    x = re.sub(r"Charlotte County Florida", "Charlotte", x)
    x = re.sub(r"Charlotte, N[.]C[.]", "Charlotte", x)
    x = re.sub(r"Charlotte, NC [|] KÌ¦ln, NRW", "Charlotte", x)
    x = re.sub(r"Charlotte, North Carolina", "Charlotte", x)
    x = re.sub(r"CharlotteCounty Florida", "Charlotte", x)
    x = re.sub(r"CharlotteNC", "Charlotte", x)
    x = re.sub(r"Charlottetown", "Charlotte", x)
    x = re.sub(r"Charlotte[|]Charlotte", "Charlotte", x)

    x = re.sub(r"Brooklyn, NY", "Brooklyn", x)

    x = re.sub(r"Brasil, Fortaleza ce", "Brazil", x)
    x = re.sub(r"Brasil,SP", "Brazil", x)
    x = re.sub(r"BrasÌ_lia", "Brazil", x)
    x = re.sub(r"Brazil", "Brazil", x)
    x = re.sub(r"Brazil ", "Brazil", x)

    x = re.sub(r"Calgary, Alberta", "Calgary", x)
    x = re.sub(r"Calgary, AB", "Calgary", x)
    x = re.sub(r"Calgary, AB, Canada", "Calgary", x)
    x = re.sub(r"Calgary/Airdrie/RedDeer/AB", "Calgary", x)
    x = re.sub(r"Calgary,AB, Canada", "Calgary", x)
    x = re.sub(r"Calgary, Canada", "Calgary", x)
    x = re.sub(r"Calgary, Alberta, Canada", "Calgary", x)
    x = re.sub(r"MontrÌ©al", "Montreal", x)
   
    x = re.sub(r"Alberta ", "Alberta", x)
    x = re.sub(r"Alberta Pack", "Alberta", x)
    x = re.sub(r"Alberta Pack", "Alberta", x)
    x = re.sub(r"Alberta [|] Sask[.] [|] Montana", "Alberta", x)

    x = re.sub(r"Morioh, Japan", "Japan", x)
    x = re.sub(r"Orlando, FL ", "Orlando", x)
    x = re.sub(r"Portland, OR", "Portland", x)
   
    x = re.sub(r"Afghanistan, USA", "Afghanistan", x)
    x = re.sub(r"Abuja", "Abuja", x)
    x = re.sub(r"Abuja,Nigeria", "Abuja", x)
    x = re.sub(r"Abuja", "Nigeria", x)

    x = re.sub(r"Alabama, USA", "Alabama", x)
    x = re.sub(r"Alameda and Pleasanton, CA", "Alameda", x)
    x = re.sub(r"Alameda, CA", "Alameda", x)

    x = re.sub(r"Alaska, USA", "Alaska", x)

    x = re.sub(r"Albuquerque New Mexico", "Albuquerque", x)

    x = re.sub(r"Alexandria, VA", "Alexandria", x)
    x = re.sub(r"Alexandria, VA, USA", "Alexandria", x)

    x = re.sub(r"All around the world", "worldwide!", x)
    x = re.sub(r"All around the world", "worldwide!", x)
    x = re.sub(r"All around the world baby", "worldwide!", x)
    x = re.sub(r"All around the world[!]", "worldwide!", x)
    x = re.sub(r"All Around the World baby", "worldwide!", x)
    x = re.sub(r"All Around the World[!]", "worldwide!", x)

    x = re.sub(r"Alicante, Spain", "Alicante", x)
    x = re.sub(r"Alicante, Valencia", "Alicante", x)

    return x

#Nous effectuerons simultanément les changements sur le train et sur le test pour faciliter les prochaines parties de l'étude:

#on train
df_train['location_clean']=df_train['location'].apply(lambda x: correct_location(x))
print(df_train['location_clean'].tail(25))

#on test
df_test['location_clean'] = df_test['location'].apply(lambda x: correct_location(x))

#on train

#Il y a certaines villes qui s'écrivent ainsi : "New York City, US" on sépare avant et après la virgule et on ne garde que la première partie "New york City"
df_train['location_clean']=df_train['location_clean'].apply(lambda x: x.split(',')[0])
#Idem avec le point
df_train['location_clean']=df_train['location_clean'].apply(lambda x: x.split('.')[0])
#On supprime les espaces au début et à la fin 
df_train['location_clean']=df_train['location_clean'].apply(lambda x: x.strip())
#Nous mettons tout en minuscules car nous avons trouvé des doublons en minuscules
df_train['location_clean']=df_train['location_clean'].str.lower()
#Nous ne gardons que les lettres, tous les caractères non alpha sont supprimés, y compris les chiffres
df_train['location_clean']=df_train['location_clean'].apply(lambda x: "".join(ch for ch in x if ch.isalpha() or ch == ' '))
print(df_train['location_clean'].tail(25))

#on test
df_test['location_clean'] = df_test['location_clean'].apply(lambda x: x.split(',')[0])
df_test['location_clean'] = df_test['location_clean'].apply(lambda x: x.split('.')[0])
df_test['location_clean'] = df_test['location_clean'].apply(lambda x: x.strip())
df_test['location_clean'] = df_test['location_clean'].str.lower()
df_test['location_clean'] = df_test['location_clean'].apply(lambda x: "".join(ch for ch in x if ch.isalpha() or ch == ' '))

df_train['location_clean'].value_counts()[0:20]

#Visualisons à nouveau les données
#Let's take the rest of the 20 most frequent keywords
top20 = df_train['location_clean'].value_counts()[0:20].index.tolist()
#We take all the locations and targets if they are in the top20 list
df_top20 = [(location_clean,target) for location_clean,target in zip(df_train['location_clean'], df_train['target']) if location_clean in top20]
df_top20 = pd.DataFrame(df_top20, columns = ['location_clean', 'target'])

sns.set_theme(style="ticks")

fig,ax = plt.subplots(1,1, figsize = (20,6))

g20 = sns.histplot(data = df_top20, x = 'location_clean', hue = 'target', multiple = 'dodge', shrink = 0.5, ax = ax)
g20.set_xticklabels(labels = top20, rotation=45)
g20.set_xlabel('')
g20.set_title('Top 20 location count by nature of tweet', fontsize=20)
g20.legend(['Disaster','Not Disaster'],loc="upper right")

plt.show()

#Comme on peut le voir les données sont plus propres, mais comme nous l'avons dit plus tôt nous devons nous 
#débarrasser des "non localisés" car il y en a trop, cela va biaiser les données. De plus, il n'est pas très 
#utile de conserver des emplacements qui ne sont utilisés que pour quelques tweets. Nous décidons donc de ne 
#garder que les emplacements qui sont utilisés pour plus de 5 tweets

#faire la liste des emplacements utilisés moins de 5 fois. Nous supprimerons ensuite ces mots-clés et les "non localisés"
#on train 
mask = df_train['location_clean'].value_counts() <5
under_5 = df_train['location_clean'].value_counts()[mask].index.to_list()
df_train['location_clean'] = df_train['location_clean'].apply(lambda x: '' if x in under_5 or x =="unlocated" else x)
df_train['location_clean']=df_train['location_clean'].replace('', np.nan)
df_train=df_train.dropna(subset=['location_clean'])
print(df_train.tail(5))


#on test
mask = df_test['location_clean'].value_counts() <5
under_5 = df_test['location_clean'].value_counts()[mask].index.to_list()
df_test['location_clean'] = df_test['location_clean'].apply(lambda x: '' if x in under_5 or x =="unlocated" else x)
df_test['location_clean']=df_test['location_clean'].replace('', np.nan)
df_test=df_test.dropna(subset=['location_clean'])

df_train.shape

"""III- Nettoyage du texte"""

import spacy
spacy.__version__
!python -m spacy download en_core_web_md -q

# Import English using en_core_web_md.load()
import en_core_web_md
nlp = en_core_web_md.load()
# avec nlp je peux traiter une chaine de caractères

# Import Stop words 
from spacy.lang.en.stop_words import STOP_WORDS

#La tâche suivante consiste à créer une fonction de normalisation de texte spécifique au contenu trouvé dans les tweets. 
#Outre la ponctuation, il existe des URL, des abréviations, des entités, des retweets, des chiffres, des mots vides et bien sûr 
#des emojis. Les mots de chaque tweet seront également lemmatisés ou réduits à leur forme racine à l'aide de la bibliothèque spaCy. 
#Nous allons commencer par créer un dictionnaire de recherche avec des abréviations courantes de phrases Twitter. Les termes de Tweet 
#qui correspondent aux clés du dictionnaire de recherche seront étendus à leur forme non abrégée.
lookup_dict = {
  'abt' : 'about',
  'afaik' : 'as far as i know',
  'bc' : 'because',
  'bfn' : 'bye for now',
  'bgd' : 'background',
  'bh' : 'blockhead',
  'br' : 'best regards',
  'btw' : 'by the way',
  'cc': 'carbon copy',
  'chk' : 'check',
  'dam' : 'do not annoy me',
  'dd' : 'dear daughter',
  'df': 'dear fiance',
  'ds' : 'dear son',
  'dyk' : 'did you know',
  'em': 'email',
  'ema' : 'email address',
  'ftf' : 'face to face',
  'fb' : 'facebook',
  'ff' : 'follow friday', 
  'fotd' : 'find of the day',
  'ftw': 'for the win',
  'fwiw' : 'for what it is worth',
  'gts' : 'guess the song',
  'hagn' : 'have a good night',
  'hand' : 'have a nice day',
  'hotd' : 'headline of the day',
  'ht' : 'heard through',
  'hth' : 'hope that helps',
  'ic' : 'i see',
  'icymi' : 'in case you missed it',
  'idk' : 'i do not know',
  'ig': 'instagram',
  'iirc' : 'if i remember correctly',
  'imho' : 'in my humble opinion',
  'imo' : 'in my opinion',
  'irl' : 'in real life',
  'iwsn' : 'i want sex now',
  'jk' : 'just kidding',
  'jsyk' : 'just so you know',
  'jv' : 'joint venture',
  'kk' : 'cool cool',
  'kyso' : 'knock your socks off',
  'lmao' : 'laugh my ass off',
  'lmk' : 'let me know', 
  'lo' : 'little one',
  'lol' : 'laugh out loud',
  'mm' : 'music monday',
  'mirl' : 'meet in real life',
  'mrjn' : 'marijuana',
  'nbd' : 'no big deal',
  'nct' : 'nobody cares though',
  'njoy' : 'enjoy',
  'nsfw' : 'not safe for work',
  'nts' : 'note to self',
  'oh' : 'overheard',
  'omg': 'oh my god',
  'oomf' : 'one of my friends',
  'orly' : 'oh really',
  'plmk' : 'please let me know',
  'pnp' : 'party and play', 
  'qotd' : 'quote of the day',
  're' : 'in reply to in regards to',
  'rtq' : 'read the question',
  'rt' : 'retweet',
  'sfw' : 'safe for work',
  'smdh' : 'shaking my damn head', 
  'smh' : 'shaking my head',
  'so' : 'significant other',
  'srs' : 'serious',
  'tftf' : 'thanks for the follow',
  'tftt' : 'thanks for this tweet',
  'tj' : 'tweetjack',
  'tl' : 'timeline',
  'tldr' : 'too long did not read',
  'tmb' : 'tweet me back',
  'tt' : 'trending topic',
  'ty' : 'thank you',
  'tyia' : 'thank you in advance',
  'tyt' : 'take your time',
  'tyvw' : 'thank you very much',
  'w': 'with', 
  'wtv' : 'whatever',
  'ygtr' : 'you got that right',
  'ykwim' : 'you know what i mean',
  'ykyat' : 'you know you are addicted to',
  'ymmv' : 'your mileage may vary',
  'yolo' : 'you only live once',
  'yoyo' : 'you are on your own',
  'yt': 'youtube',
  'yw' : 'you are welcome',
  'zomg' : 'oh my god to the maximum'
}

def abbrev_conversion(text):
    words = text.split() 
    abbrevs_removed = [] 
    
    for i in words:
        if i in lookup_dict:
            i = lookup_dict[i]
        abbrevs_removed.append(i)
            
    return ' '.join(abbrevs_removed)
# on créée une fonction qui nous permet de transformer les phrases abrégées en phrases non abrégées

#on train 
#pour supprimer les urls 
df_train["text_clean"] = df_train["text"].apply(lambda x: re.sub(re.compile(r'(?:\@|http?\://|https?\://|www)\S+'), '', x) if pd.isna(x) != True else x) 
#On supprimer tous les caractères non alphanumériques à l'exception des espaces 
df_train["text_clean"] = df_train["text_clean"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==" "))
#On va transformer ttes les phrases abrégées en non abrgées mais on ne prend pas en compte les NAN.
df_train["text_clean"]= df_train["text_clean"].apply(lambda x: abbrev_conversion(x) if pd.isna(x) != True else x)
#On supprime les doubles espaces et les espaces au début et à la fin des chaînes et on transforme en miniscule
df_train["text_clean"] =df_train["text_clean"].apply(lambda x: x.replace(" +"," ").lower().strip())
#On supprime les mots vides et remplace chaque mot par son lemme
#pour chaque token ds nlp appliqué à x qui représente un élément dans ma colonne on va chercher les lemmas mais
#uniquement si le texte associé à mon token n'est pas ds les stop_words et que le lemma associé à mon token n'est pas dans les stop_words non plus
df_train["text_clean"]= df_train["text_clean"].apply(lambda x: " ".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))
print(df_train.head())

#on test
df_test["text_clean"] = df_test["text"].apply(lambda x: re.sub(re.compile(r'(?:\@|http?\://|https?\://|www)\S+'), '', x) if pd.isna(x) != True else x)  
df_test["text_clean"] = df_test["text_clean"].apply(lambda x:''.join(ch for ch in x if ch.isalnum() or ch==" "))
df_test["text_clean"]= df_test["text_clean"].apply(lambda x: abbrev_conversion(x) if pd.isna(x) != True else x)
df_test["text_clean"] =df_test["text_clean"].apply(lambda x: x.replace(" +"," ").lower().strip())
df_test["text_clean"]= df_test["text_clean"].apply(lambda x: " ".join([token.lemma_ for token in nlp(x) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)]))

df_train.shape

"""Le modèle prédit si un tweet donné parle d'un véritable désastre ou non. Si oui, il prédit un 1. Sinon, il prédit un 0."""

# Word cloud of text in disaster and non-disaster tweets

# Cleaned dataframe of disaster
df_true = df_train[df_train.target == 1]

text_true = " ".join(txt for txt in df_true['text'])

text_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)
plt.axis("off")
plt.imshow(text_cloud, interpolation='bilinear')

#Words in disaster tweets:- disaster, new, fire, via, year, suicide, police, home,killed, crash, storm, etc.

# Cleaned dataframe of disaster
df_true = df_train[df_train.target == 0]

text_true = " ".join(txt for txt in df_true['text'])

text_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)
plt.axis("off")
plt.imshow(text_cloud, interpolation='bilinear')

#Words in non-disaster tweets:- new, love, one, time, now, body, people,good, etc.

import numpy as np
tokenizer = tf.keras.preprocessing.text.Tokenizer() # instanciate the tokenizer pr garder les mots les plus fréquents
tokenizer.fit_on_texts(df_train.text_clean)#il va créer son tableau de correspondance pour remplacer les textes en liste d'indices 
df_train["text_encoded"] = tokenizer.texts_to_sequences(df_train.text_clean)# pour remplacer le texte en liste d'indices 

df_train["len_text"] = df_train["text_encoded"].apply(lambda x: len(x))
df_train= df_train[df_train["len_text"]!=0] 
df_train

#On va nettoyer notre texte à l'aide de spacy puis on va encoder le texte en le transformant en liste d'indices
#pr ceci on utilise le tenserflow tokenizer

## On fait de même sur le set de test, sauf qu'on ne fit pas sur le tokenizer 
df_test["text_encoded"] = tokenizer.texts_to_sequences(df_test.text_clean)
df_test["len_text"] =df_test["text_encoded"].apply(lambda x: len(x))
df_test=df_test[df_test["len_text"]!=0]

# on rajoute du padding pour avoir la même longueur dans tte les phrase 
train_pad = tf.keras.preprocessing.sequence.pad_sequences(df_train.text_encoded, padding="post")
test_pad = tf.keras.preprocessing.sequence.pad_sequences(df_test.text_encoded, padding="post")

seed=123 # pr avoir le mm comportement qui se reproduit à chaque fois c'est comme un random state
train_x, val_x, train_y, val_y = train_test_split(
    train_pad,
    df_train.target,
    test_size=0.2,
    random_state=seed
)

print(train_x.shape)
print(train_y.shape)
print(val_x.shape)
print(val_y.shape)

# on transforme notre dataset en jeu de données tensoriel dans lequel on va mettre un tuple (1 er éléméent nos données, 2ème élément notre vble cible ) 
train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))
val_ds=tf.data.Dataset.from_tensor_slices((val_x, val_y))
test_ds = tf.data.Dataset.from_tensor_slices(test_pad)

next(iter(train_ds))
# c'est un élément de mon tenseur dataset j'ai une séquence de 25 mots avec plein de 0 padding et un tenseur qui 
# contient ma vble cible

train_ds = train_ds.shuffle(len(train_ds)).batch(64)
val_ds=val_ds.shuffle(len(val_ds)).batch(64)
test_ds = test_ds.batch(64)
# on applique le shuffle pr qu'à la fin de chaque époque les données se remélangent et on les organisent par
# batch de 64 phrases

len(tokenizer.word_index)

"""IV-**Modeling**"""

from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, GRU, LSTM,GlobalAveragePooling1D, Dropout

# c'est le nbre de mots qu'on a séléctionné dans notre tokenizer 
vocab_size=len(tokenizer.word_index)
model = tf.keras.Sequential([
                  # Couche d'Input Word Embedding(chaque indice va être représenté par un vecteur pour avoir une représentation plus riche qui nous donnera la possibilité d'analyser plus finement les mots de nos textes)         
                  tf.keras.layers.Embedding(vocab_size+1, 8, input_shape=[train_pad.shape[1],],name="embedding"), # chaque mot va être représeenté par un vecteur de 8 valeurs
                  # Gobal average pooling
                  tf.keras.layers.GlobalAveragePooling1D(),# pr supprimer la dimension séquentielle, il va faire un résumer

                  # Couche Dense classique
                  tf.keras.layers.Dense(16, activation='relu'),
                  tf.keras.layers.Dense(8, activation='relu'),

                  # Couche de sortie avec le nombre de neurones en sortie égale au nombre de classe avec fonction softmax
                  tf.keras.layers.Dense(1, activation="sigmoid") # c'est la couche de prédiction avec une classification binaire 
])
# input_dim c'est le nbre de mot utilisé et input shape c'est le nbr de mots dans mes phrases, la longueur de mes phrases

model.summary()
# None c'est la batch size 25 mots par phrases et chaque est représenté par 32 valeurs, on fait le GA on se
# retrouve avec 8 valeurs

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])
# Vu qu'on est en classification

# Entrainement du modèle 
history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=20, 
                  )

import matplotlib.pyplot as plt

# Visualization of the training process on the loss function 
plt.plot(history.history["loss"], color="b")
plt.plot(history.history["val_loss"], color="r")
plt.ylabel("loss")
plt.xlabel("Epochs")
plt.show()

# Visualization of accuracy training 
plt.plot(history.history["accuracy"], color="b")
plt.plot(history.history["val_accuracy"], color="r")
plt.ylabel("accuracy")
plt.xlabel("Epochs")
plt.show()

# La faiblesse des neuronnes dense pr analyser des séquences c'est qu'il traite chaque mot de manière indépendante
# l'information d'un mot n'influence pas l'autre mot

vocab_size=len(tokenizer.word_index)
model = tf.keras.Sequential([
                  # Word Embedding layer           
                  Embedding(vocab_size+1,64, input_shape=[train_pad.shape[1],],name="embedding"),
                  SimpleRNN(units=32, return_sequences=False), # returns the last output
                  # Dense layers once the data is flat
                  Dense(16, activation='relu'),
                  # output layer with as many neurons as the number of classes
                  # for the target variable and softmax activation
                  Dropout(0.5),
                  Dense(1, activation="sigmoid")
])
#le choix de la sigmoid parce que notre target va avoir deux valeurs 0 ou 1 s'il y a catastrophe 
# vocab_size c'est le nbre de mot dont on dispose +1 pr le padding 
# input_shape : le nombre max de mot dans la phrase

model.summary()

model.compile(optimizer='adam', # c'est l'algorithme de descente de gradient spécifique qu'on veut choisir pr l'entraîner
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Entrainement du modèle 
history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=20, 
                  )

import matplotlib.pyplot as plt

# Visualization of the training process on the loss function 
plt.plot(history.history["loss"], color="b")
plt.plot(history.history["val_loss"], color="r")
plt.ylabel("loss")
plt.xlabel("Epochs")
plt.show()
# comme le schéma le montre on commence à overfitter depuis le début la val loss n'arrête pas d'augmenter

# Visualization of accuracy training 
plt.plot(history.history["accuracy"], color="b")
plt.plot(history.history["val_accuracy"], color="r")
plt.ylabel("accuracy")
plt.xlabel("Epochs")
plt.show()

# L'inconvenient du SRNN c'est qu'il n'est pas efficace sur les phrases longues, la fonction d'activation qu'on utilise 
# le plus souvent c'est une tangente hyperbolique comme on va dérivé plusieurs tangente hyperbolique qui va être comprise
# entre 0 et 1 donc plus on va multilplier les dérivés plus on va réduire la valeur du gradient, on va avoir un problème
# de vanishing gradient on va avoir bcp de mal à entrainer le modèle et donc a bien prédire

model.save("model_simpleRNN.h5")

from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, GRU, LSTM

vocab_size=len(tokenizer.word_index)
model_gru = tf.keras.Sequential([
                  Embedding(vocab_size+1,64, input_shape=[train_pad.shape[1],],name="embedding"),
                  GRU(units=32, return_sequences=True), # maintains the sequential nature
                  tf.keras.layers.Bidirectional(GRU(units=16, return_sequences=False)), # returns the last output
                  Dense(16, activation='relu'),
                  Dropout(0.5),
                  Dense(1, activation="sigmoid")
])

model_gru.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Entrainement du modèle 
history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=20, 
                  )

# Visualization of the training process on the loss function 
plt.plot(history.history["loss"], color="b")
plt.plot(history.history["val_loss"], color="r")
plt.ylabel("loss")
plt.xlabel("Epochs")
plt.show()

# Visualization of accuracy training 
plt.plot(history.history["accuracy"], color="b")
plt.plot(history.history["val_accuracy"], color="r")
plt.ylabel("accuracy")
plt.xlabel("Epochs")
plt.show()

# Pour la GRU je vais avoir la reset gate qui répond à la question est ce qui a dans ma mémoire va être utile 
# pour créer ma nouvelle info, la update gate en deux parties pr créer mon nouvel output je vais garder quelle 
# proportion de ma mémoire et quelle proportion de la nouvelle info. Le neuronne est capable de choisir s'il veut
# passer une proportion de l'ancienne info et une proportion de la noubvelle info ou bien il peut censurer l'une et pas du tout 
# l'autre et parfois ça sera l'inverse donc garder tt l'ancien et pas le nouveau ou bien l'inverse

model_gru.save("model_gru.h5")

from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, GRU, LSTM

vocab_size=len(tokenizer.word_index)
model_lstm= tf.keras.Sequential([
                  Embedding(vocab_size+1,64, input_shape=[train_pad.shape[1],],name="embedding"),
                  LSTM(units=32, return_sequences=True), # maintains the sequential nature
                  tf.keras.layers.Bidirectional(LSTM(units=16, return_sequences=False)), # returns the last output
                  Dense(16, activation='relu'),
                  Dropout(0.5),
                  Dense(1, activation="sigmoid")
])
# Bidirectional ça permet de dupliquer le nbre de neurones la moitié des neurones va lire la séquence de gauche
# à droite et l'autre moitié va la lire de droite à gauche

model_lstm.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Entrainement du modèle 
history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=20, 
                  )

# Visualization of the training process on the loss function 
plt.plot(history.history["loss"], color="b")
plt.plot(history.history["val_loss"], color="r")
plt.ylabel("loss")
plt.xlabel("Epochs")
plt.show()

# Visualization of accuracy training 
plt.plot(history.history["accuracy"], color="b")
plt.plot(history.history["val_accuracy"], color="r")
plt.ylabel("accuracy")
plt.xlabel("Epochs")
plt.show()

# Lstm ce sont des neurones qui sont capables d'utiliser à la fois de la mémoire de court terme et de la mémoire 
# de long terme on a deux mémoires une mémoire interne au neurone c'est cell state et une autre mémoire qui va 
# communiquer  avec les autres neurones et qui est le hidden state

model_lstm.save("model_gru.h5")

pred=model.predict(train_pad)

target=df_train.target

df_eval=pd.DataFrame({"pred":pred.squeeze(),"target":target,"text":df_train.text_clean})
df_eval

# On prédit si un tweet donné parle d'un véritable désastre ou non. Si oui on prédit un 1 sinon on prédit un 0.
# On peut dire que notre modèle fait de bonnes prédictions puisque pr cry set ablaze et plus look sky night ablaze 
# il déclare que ce sont de fausses déclarations